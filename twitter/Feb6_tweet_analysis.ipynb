{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9adc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime\n",
    "from datetime import time\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce83fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file1 = r\"tweets_tr_help_formatted.csv\"\n",
    "input_file1_name = \"tr_help_formatted\"\n",
    "input_file2 = r\"tweets_tr.csv\"\n",
    "input_file2_name = \"tweets_tr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0305be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loads two CSV Tweet datasets, \n",
    "cleans and analyzes their text to count top words, post times, dates, locations, and hashtags (both by frequency and engagement), \n",
    "and prints them\n",
    "\"\"\"\n",
    "\n",
    "text_column_name = 'content'\n",
    "\n",
    "top_n = 100\n",
    "\n",
    "def clean_text(text):\n",
    "    #Cleans a string by converting it to lowercase, removing common Twitter artifacts, and stripping punctuation and symbols.\n",
    "\n",
    "    cleaned_text = re.sub(r'http\\S+|@\\w+|#\\w+', '', text, flags=re.MULTILINE)\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
    "    return cleaned_text.lower()\n",
    "\n",
    "def get_word_distribution(file_path, column_name):\n",
    "    word_counts = collections.Counter()\n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            \n",
    "            if column_name not in reader.fieldnames:\n",
    "                print(f\"Error: The column '{column_name}' was not found in the file.\")\n",
    "                return None\n",
    "            \n",
    "            for row in reader:\n",
    "                text = row.get(column_name, '')\n",
    "                if text:\n",
    "                    cleaned = clean_text(text)\n",
    "                    words = cleaned.split()\n",
    "                    word_counts.update(words)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Exception\")\n",
    "        return None\n",
    "        \n",
    "    return word_counts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    word_counts1 = get_word_distribution(input_file1, text_column_name)\n",
    "    word_counts2 = get_word_distribution(input_file2, text_column_name)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        word_counts1 = get_word_distribution(input_file1, text_column_name)\n",
    "        word_counts2 = get_word_distribution(input_file2, text_column_name)\n",
    "\n",
    "        if word_counts1 and word_counts2:\n",
    "            top_words1 = word_counts1.most_common(top_n)\n",
    "            top_words2 = word_counts2.most_common(top_n)\n",
    "\n",
    "            print(f\"{'Top Words in ' + input_file1_name:<50} {'    Top Words in ' + input_file2_name:<50}\")\n",
    "            max_length = max(len(top_words1), len(top_words2))\n",
    "\n",
    "            for i in range(max_length):\n",
    "                word1, count1 = top_words1[i] if i < len(top_words1) else ('', '')\n",
    "                word2, count2 = top_words2[i] if i < len(top_words2) else ('', '')\n",
    "                print(f\"{str(word1) + ' (' + str(count1) + ')':<60} {str(word2) + ' (' + str(count2) + ')':<70}\")\n",
    "                \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46431793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour       Posts in tweets_tr_help_formatted.csv (%) Posts in tweets_tr.csv (%)    \n",
      "0                                  1.31                        1.40%\n",
      "1                                  1.24                        1.33%\n",
      "2                                  0.82                        1.08%\n",
      "3                                  0.73                        0.99%\n",
      "4                                  1.15                        1.54%\n",
      "5                                  1.31                        2.16%\n",
      "6                                  1.88                        2.56%\n",
      "7                                  2.58                        3.03%\n",
      "8                                  3.53                        4.26%\n",
      "9                                  4.94                        5.66%\n",
      "10                                 5.26                        5.78%\n",
      "11                                 7.65                        6.34%\n",
      "12                                 6.24                        5.49%\n",
      "13                                 4.65                        4.64%\n",
      "14                                 5.47                        5.41%\n",
      "15                                 4.91                        5.33%\n",
      "16                                 5.81                        5.90%\n",
      "17                                 6.30                        6.04%\n",
      "18                                 6.88                        6.23%\n",
      "19                                 6.73                        6.18%\n",
      "20                                 8.09                        7.25%\n",
      "21                                 5.43                        4.94%\n",
      "22                                 4.82                        4.26%\n",
      "23                                 2.26                        2.19%\n",
      "\n",
      "Analysis complete.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reads each fileâ€™s date column, counts posts per hour, calculates the percentage for each hour, \n",
    "and prints an hourly posting distribution comparison for the two datasets\n",
    "\"\"\"\n",
    "\n",
    "date_column_name = 'date'\n",
    "\n",
    "def get_hour_distribution(file_path, column_name):\n",
    "    hour_counts = collections.Counter()\n",
    "    total_posts = 0\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            \n",
    "            if column_name not in reader.fieldnames:\n",
    "                print(f\"Error: The column '{column_name}' was not found in '{file_path}'.\")\n",
    "                return None, None\n",
    "            \n",
    "            for row in reader:\n",
    "                date_str = row.get(column_name, '')\n",
    "                if date_str:\n",
    "                    try:\n",
    "                        dt_object = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "                        hour = dt_object.hour\n",
    "                        hour_counts[hour] += 1\n",
    "                        total_posts += 1\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Value error\")\n",
    "                        continue\n",
    "                        \n",
    "\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "        \n",
    "    return hour_counts, total_posts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hour_counts1, total_posts1 = get_hour_distribution(input_file1, date_column_name)\n",
    "    hour_counts2, total_posts2 = get_hour_distribution(input_file2, date_column_name)\n",
    "\n",
    "    if hour_counts1 and hour_counts2 and total_posts1 and total_posts2:\n",
    "        print(f\"{'Hour':<10} {'Posts in ' + input_file1 + ' (%)':<30} {'Posts in ' + input_file2 + ' (%)':<30}\")\n",
    "    \n",
    "        for hour in range(24):\n",
    "            percentage1 = (hour_counts1.get(hour, 0) / total_posts1) * 100\n",
    "            percentage2 = (hour_counts2.get(hour, 0) / total_posts2) * 100\n",
    "\n",
    "            print(f\"{hour:<10} {percentage1:>28.2f}{percentage2:>28.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee1ac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date            Posts in tweets_tr_help_formatted.csv (%) |     Posts in tweets_tr.csv (%)         \n",
      "2023-02-06                                  43.45%                              38.81%\n",
      "2023-02-07                                  35.03%                              30.22%\n",
      "2023-02-08                                  16.43%                              16.50%\n",
      "2023-02-09                                   5.05%                               9.24%\n",
      "2023-02-10                                   0.00%                               1.77%\n",
      "2023-02-11                                   0.02%                               1.20%\n",
      "2023-02-12                                   0.00%                               1.86%\n",
      "2023-02-13                                   0.00%                               0.09%\n",
      "2023-02-14                                   0.00%                               0.03%\n",
      "2023-02-15                                   0.00%                               0.05%\n",
      "2023-02-16                                   0.00%                               0.04%\n",
      "2023-02-17                                   0.00%                               0.03%\n",
      "2023-02-18                                   0.00%                               0.03%\n",
      "2023-02-19                                   0.00%                               0.01%\n",
      "2023-02-20                                   0.02%                               0.12%\n",
      "2023-02-21                                   0.00%                               0.01%\n",
      "Total posts in 'tweets_tr_help_formatted.csv': 43740\n",
      "Total posts in 'tweets_tr.csv': 140532\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reads each fileâ€™s date column, counts posts per calendar day, calculates daily posting percentages, \n",
    "and prints a day-by-day comparison along with total post counts\n",
    "\"\"\"\n",
    "\n",
    "date_column_name = 'date'\n",
    "\n",
    "def get_date_distribution(file_path, column_name):\n",
    "    date_counts = collections.Counter()\n",
    "    total_posts = 0\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            \n",
    "            if column_name not in reader.fieldnames:\n",
    "                print(f\"Error: The column '{column_name}' was not found in '{file_path}'.\")\n",
    "                return None, None\n",
    "            \n",
    "            for row in reader:\n",
    "                date_str = row.get(column_name, '')\n",
    "                if date_str:\n",
    "                    try:\n",
    "                        dt_object = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "                        date_obj = dt_object.date()\n",
    "                        date_counts[date_obj] += 1\n",
    "                        total_posts += 1\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Warning: Could not parse date '{date_str}' in '{file_path}'. Error: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"Exception\")\n",
    "        return None, None\n",
    "        \n",
    "    return date_counts, total_posts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    date_counts1, total_posts1 = get_date_distribution(input_file1, date_column_name)\n",
    "    date_counts2, total_posts2 = get_date_distribution(input_file2, date_column_name)\n",
    "\n",
    "    if date_counts1 and date_counts2 and total_posts1 and total_posts2:\n",
    "\n",
    "        all_dates = sorted(list(date_counts1.keys() | date_counts2.keys()))\n",
    "        print(f\"{'Date':<15} {'Posts in ' + input_file1 + ' (%)':<35} {'|':<5} {'Posts in ' + input_file2 + ' (%)':<35}\")\n",
    "\n",
    "        for date_obj in all_dates:\n",
    "            percentage1 = (date_counts1.get(date_obj, 0) / total_posts1) * 100\n",
    "            percentage2 = (date_counts2.get(date_obj, 0) / total_posts2) * 100\n",
    "\n",
    "            print(f\"{str(date_obj):<15} {percentage1:>33.2f}%  {percentage2:>33.2f}%\")\n",
    "\n",
    "        print(f\"Total posts in '{input_file1}': {total_posts1}\")\n",
    "        print(f\"Total posts in '{input_file2}': {total_posts2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8717b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cleans, and concatenates all text from each file, computes TF-IDF scores for every word, \n",
    "and prints the top-scoring terms from each dataset\n",
    "\"\"\"\n",
    "text_column_name = 'content'\n",
    "\n",
    "top_n = 20\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'http\\S+|@\\w+|#\\w+', '', text, flags=re.MULTILINE)\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
    "    return cleaned_text.lower()\n",
    "\n",
    "def get_text_from_file(file_path, column_name):\n",
    "\n",
    "    all_text = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            \n",
    "            if column_name not in reader.fieldnames:\n",
    "                print(f\"Error: The column '{column_name}' was not found in '{file_path}'.\")\n",
    "                return None\n",
    "            \n",
    "            for row in reader:\n",
    "                text = row.get(column_name, '')\n",
    "                if text:\n",
    "                    cleaned = clean_text(text)\n",
    "                    all_text.append(cleaned)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Exception\")\n",
    "        return None\n",
    "        \n",
    "    return \" \".join(all_text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    document1 = get_text_from_file(input_file1, text_column_name)\n",
    "    document2 = get_text_from_file(input_file2, text_column_name)\n",
    "\n",
    "    if document1 and document2:\n",
    "        corpus = [document1, document2]\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        \n",
    "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        tfidf_scores1 = tfidf_matrix[0].toarray().flatten()\n",
    "        top_words1 = sorted(zip(feature_names, tfidf_scores1), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        tfidf_scores2 = tfidf_matrix[1].toarray().flatten()\n",
    "        top_words2 = sorted(zip(feature_names, tfidf_scores2), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "        print(f\"{'Top TF-IDF Words in ' + input_file1:<50} {'Top TF-IDF Words in ' + input_file2:<50}\")\n",
    "        \n",
    "        max_length = max(len(top_words1), len(top_words2))\n",
    "        for i in range(max_length):\n",
    "            word1, score1 = top_words1[i] if i < len(top_words1) else ('', '')\n",
    "            word2, score2 = top_words2[i] if i < len(top_words2) else ('', '')\n",
    "            \n",
    "            print(f\"{str(word1):<25} ({str(f'{score1:.4f}'):<15}) {str(word2):<25} ({str(f'{score2:.4f}'):<15})\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233612cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checks each file for coordinate or place data in the specified columns, counts how many rows contain each type, \n",
    "and prints total posts, and their location data\n",
    "\"\"\"\n",
    "want_coord = \"coordinates\" \n",
    "want_place = \"place\"       \n",
    "\n",
    "EMPTY_TOKENS = {\"\", \"none\", \"null\", \"nan\", \"[]\", \"{}\", \"na\", \"n/a\"}\n",
    "\n",
    "# Patterns for coordinates\n",
    "pat_coords1 = re.compile(r\"Coordinates\\(\\s*longitude=([\\-+]?\\d+(?:\\.\\d+)?),\\s*latitude=([\\-+]?\\d+(?:\\.\\d+)?)\\s*\\)\", re.I)\n",
    "pat_coords2 = re.compile(r\"Coordinates\\(\\s*latitude=([\\-+]?\\d+(?:\\.\\d+)?),\\s*longitude=([\\-+]?\\d+(?:\\.\\d+)?)\\s*\\)\", re.I)\n",
    "pat_plain   = re.compile(r\"^\\s*([\\-+]?\\d+(?:\\.\\d+)?)\\s*,\\s*([\\-+]?\\d+(?:\\.\\d+)?)\\s*$\")\n",
    "\n",
    "def norm(s):\n",
    "    return (s or \"\").strip()\n",
    "\n",
    "def is_empty_val(s):\n",
    "    return norm(s).lower() in EMPTY_TOKENS\n",
    "\n",
    "def has_coords(s):\n",
    "    s = norm(s)\n",
    "    if not s or is_empty_val(s): return False\n",
    "    if pat_coords1.search(s) or pat_coords2.search(s): return True\n",
    "    if pat_plain.match(s): return True\n",
    "    return False\n",
    "\n",
    "def has_place(s):\n",
    "    s = norm(s)\n",
    "    if not s or is_empty_val(s): return False\n",
    "\n",
    "    if s.startswith(\"Place(\") or \"fullName='\" in s or \"name='\" in s:\n",
    "        return True\n",
    "\n",
    "    return any(ch.isalpha() for ch in s)\n",
    "\n",
    "def find_column(fieldnames, target):\n",
    "    target_n = target.strip().lower()\n",
    "    for fn in fieldnames:\n",
    "        if fn and fn.strip().lower() == target_n:\n",
    "            return fn\n",
    "    return None\n",
    "\n",
    "def analyze(file_path):\n",
    "    total = with_coords = with_place_ = with_any = 0\n",
    "    with open(file_path, \"r\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if not reader.fieldnames:\n",
    "            return (0, 0, 0, 0)\n",
    "        coord_col = find_column(reader.fieldnames, want_coord)\n",
    "        place_col = find_column(reader.fieldnames, want_place)\n",
    "        if not coord_col and not place_col:\n",
    "            return (0, 0, 0, 0)\n",
    "\n",
    "        for row in reader:\n",
    "            total += 1\n",
    "            cval = row.get(coord_col, \"\") if coord_col else \"\"\n",
    "            pval = row.get(place_col, \"\") if place_col else \"\"\n",
    "\n",
    "            hc = has_coords(cval)\n",
    "            hp = has_place(pval)\n",
    "\n",
    "            if hc: with_coords += 1\n",
    "            if hp: with_place_ += 1\n",
    "            if hc or hp: with_any += 1\n",
    "\n",
    "    return (total, with_coords, with_place_, with_any)\n",
    "\n",
    "def pct(part, whole):\n",
    "    return f\"{(part/whole*100):.2f}%\" if whole else \"0.00%\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t1, c1, p1, a1 = analyze(input_file1)\n",
    "    t2, c2, p2, a2 = analyze(input_file2)\n",
    "\n",
    "   \n",
    "    print(\"Location Analysis for \" + input_file1 + \"\\t\" + \"Location Analysis for \" + input_file2)\n",
    "    print(f\"Total Posts: {t1}\\tTotal Posts: {t2}\")\n",
    "    print(f\"With Coordinates: {c1} ({pct(c1,t1)})\\tWith Coordinates: {c2} ({pct(c2,t2)})\")\n",
    "    print(f\"With Place: {p1} ({pct(p1,t1)})\\tWith Place: {p2} ({pct(p2,t2)})\")\n",
    "    print(f\"With Any Location: {a1} ({pct(a1,t1)})\\tWith Any Location: {a2} ({pct(a2,t2)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Counts how many times each place value appears in the given CSV column for both files, \n",
    "then prints the most common locations along with the total number of unique locations in each dataset\n",
    "\"\"\"\n",
    "\n",
    "def get_location_distribution(file_path, place_col):\n",
    "    location_counts = collections.Counter()\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "\n",
    "            for row in reader:\n",
    "                place = row.get(place_col)\n",
    "    \n",
    "                if place:\n",
    "                    location_counts[place] += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception\")\n",
    "        return None\n",
    "        \n",
    "    return location_counts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    location_counts1 = get_location_distribution(input_file1, \"place\" )\n",
    "    location_counts2 = get_location_distribution(input_file2, \"place\")\n",
    "\n",
    "    if location_counts1 is not None and location_counts2 is not None:\n",
    "        top_locations1 = location_counts1.most_common()\n",
    "        top_locations2 = location_counts2.most_common()\n",
    "\n",
    "        print(f\"{'Top Locations in ' + input_file1:<50}{'Top Locations in ' + input_file2:<50}\")\n",
    "   \n",
    "        max_length = max(len(top_locations1), len(top_locations2))\n",
    "        for i in range(max_length):\n",
    "\n",
    "            location1, count1 = top_locations1[i] if i < len(top_locations1) else ('', '')\n",
    "            location2, count2 = top_locations2[i] if i < len(top_locations2) else ('', '')\n",
    "            \n",
    "            print(f\"{str(location1):<25} ({str(count1):<15}) {str(location2):<25} ({str(count2):<15})\")\n",
    "            \n",
    "        print(f\"Total unique locations in '{input_file1}': {len(location_counts1)}\")\n",
    "        print(f\"Total unique locations in '{input_file2}': {len(location_counts2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb36e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-09 17:29:51+00:00, province: Ä°stanbul, district: None, country: TR\n",
      "2023-02-09 14:42:51+00:00, province: Bretagne, district: CÃ´tes-d'Armor, country: FR\n",
      "2023-02-09 14:42:01+00:00, province: Hatay, district: None, country: TR\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;66;03m# Sleep only if we actually called the API \u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coord_cache\u001b[38;5;241m.\u001b[39mget(latlon) \u001b[38;5;241m==\u001b[39m (province, district, cc) \u001b[38;5;129;01mand\u001b[39;00m delay_s \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 94\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(delay_s)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m province:\n\u001b[0;32m     97\u001b[0m     place_cell \u001b[38;5;241m=\u001b[39m (row\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplace\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reads each row of a CSV, parses coordinates or a place string, \n",
    "reverse forward geocodes it with Nominatim to get province/district/country, \n",
    "and prints those alongside the Tweetâ€™s date\n",
    "\n",
    "WARNING: When using Nominatim, do not set delay time below 1 second,\n",
    "as this can result in your IP being banned for excessive requests\n",
    "\"\"\"\n",
    "\n",
    "user_agent = \"tweet_geo_lookup\"\n",
    "delay_s = 1  \n",
    "lang = \"tr\"    \n",
    "\n",
    "geolocator = Nominatim(user_agent=user_agent)\n",
    "\n",
    "coord_pattern = re.compile(\n",
    "    r\"longitude=([+-]?\\d+(?:\\.\\d+)?),\\s*latitude=([+-]?\\d+(?:\\.\\d+)?)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Caches to avoid repeated API calls\n",
    "coord_cache = {}      \n",
    "placename_cache = {}  \n",
    "\n",
    "def parse_coordinates(coord_str: str):\n",
    "    if not coord_str:\n",
    "        return None\n",
    "    m = coord_pattern.search(coord_str)\n",
    "    if not m:\n",
    "        return None\n",
    "    lon, lat = float(m.group(1)), float(m.group(2))\n",
    "    return (lat, lon)  \n",
    "\n",
    "def reverse_to_admin(latlon):\n",
    "    if latlon in coord_cache:\n",
    "        return coord_cache[latlon]\n",
    "    province = district = cc = None\n",
    "    try:\n",
    "        loc = geolocator.reverse(latlon, language=lang, addressdetails=True)\n",
    "        if loc and loc.raw and \"address\" in loc.raw:\n",
    "            addr = loc.raw[\"address\"]\n",
    "            # TÃ¼rkiye: province can come under 'state' or 'province'\n",
    "            province = addr.get(\"state\") or addr.get(\"province\")\n",
    "            # District can appear as 'county' or 'state_district' or 'city'\n",
    "            district = addr.get(\"state_district\") or addr.get(\"county\") or addr.get(\"city\")\n",
    "            cc = (addr.get(\"country_code\") or \"\").upper()\n",
    "    except Exception as e:\n",
    "        print(f\"reverse error for {latlon}: {e}\")\n",
    "    coord_cache[latlon] = (province, district, cc)\n",
    "    return coord_cache[latlon]\n",
    "\n",
    "def extract_place_name(place_cell: str):\n",
    "\n",
    "    if not place_cell:\n",
    "        return None\n",
    "    m = re.search(r\"fullName='([^']+)'\", place_cell)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    m2 = re.search(r\"name='([^']+)'\", place_cell)\n",
    "    if m2:\n",
    "        return m2.group(1)\n",
    "    return None\n",
    "\n",
    "def forward_place_to_admin(place_str: str):\n",
    "\n",
    "    if place_str in placename_cache:\n",
    "        return placename_cache[place_str]\n",
    "    province = district = cc = None\n",
    "    try:\n",
    "        \n",
    "        query = f\"{place_str}, TÃ¼rkiye\" if \"TÃ¼rkiye\" not in place_str else place_str\n",
    "        loc = geolocator.geocode(query, language=lang, addressdetails=True)\n",
    "        if loc and loc.raw and \"address\" in loc.raw:\n",
    "            addr = loc.raw[\"address\"]\n",
    "            province = addr.get(\"state\") or addr.get(\"province\")\n",
    "            district = addr.get(\"state_district\") or addr.get(\"county\") or addr.get(\"city\")\n",
    "            cc = (addr.get(\"country_code\") or \"\").upper()\n",
    "    except Exception as e:\n",
    "        print(\"Exception error\") \n",
    "\n",
    "    placename_cache[place_str] = (province, district, cc)\n",
    "    return placename_cache[place_str]\n",
    "\n",
    "with open(input_file1, newline=\"\", encoding=\"utf-8\") as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "\n",
    "    for row in reader:\n",
    "        province = district = cc = None\n",
    "\n",
    "        latlon = parse_coordinates((row.get(\"coordinates\") or \"\").strip())\n",
    "        if latlon:\n",
    "            province, district, cc = reverse_to_admin(latlon)\n",
    "            # Sleep only if we actually called the API \n",
    "            if coord_cache.get(latlon) == (province, district, cc) and delay_s > 0:\n",
    "                time.sleep(delay_s)\n",
    "\n",
    "        if not province:\n",
    "            place_cell = (row.get(\"place\") or \"\").strip()\n",
    "            place_name = extract_place_name(place_cell)\n",
    "            if place_name:\n",
    "                province2, district2, cc2 = forward_place_to_admin(place_name)\n",
    "                if placename_cache.get(place_name) == (province2, district2, cc2) and delay_s > 0:\n",
    "                    time.sleep(delay_s)\n",
    "                province = province or province2\n",
    "                district = district or district2\n",
    "                cc = cc or cc2\n",
    "\n",
    "        if province or district or cc:\n",
    "            print(f\"{row.get('date')}, province: {province}, district: {district}, country: {cc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa08ea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Hashtags by Engagement (likes+retweets)\n",
      "hashtag\tscore\n",
      "turkey\t1108990\n",
      "deprem\t997105\n",
      "seferberlik\t299579\n",
      "afad\t294006\n",
      "hatay\t288861\n",
      "earthquake\t239642\n",
      "hatayyardimbekliyor\t239210\n",
      "enkazaltÄ±ndayÄ±m\t195678\n",
      "ohal\t174347\n",
      "sondakikadeprem\t146940\n",
      "prayforturkey\t144520\n",
      "enkaz\t128801\n",
      "hataydeprem\t121965\n",
      "aciÌ‡l\t116100\n",
      "elbistan\t112577\n",
      "sondakika\t100478\n",
      "enkazaltiÌ‡ndayiÌ‡m\t97487\n",
      "gaziantep\t96608\n",
      "kahramanmaras\t92490\n",
      "depremoldu\t80701\n",
      "turkeyearthquake\t78092\n",
      "helpturkey\t69483\n",
      "ahbap\t67291\n",
      "acildeprem\t64440\n",
      "yardim\t63379\n",
      "iskenderun\t61290\n",
      "malatya\t56999\n",
      "oguzhanugur\t54937\n",
      "afadhatay\t51720\n",
      "haarp\t50311\n",
      "haluklevent\t43550\n",
      "afaddeprem\t42792\n",
      "turkiye\t41693\n",
      "adiyaman\t41588\n",
      "adÄ±yaman\t39102\n",
      "acil\t37723\n",
      "maras\t37331\n",
      "kahramanmaraÅŸ\t37187\n",
      "hatayiskenderun\t36540\n",
      "depremsondakika\t36463\n",
      "hataydepremi\t35555\n",
      "kilis\t33723\n",
      "osmaniye\t32782\n",
      "diyarbakÄ±r\t31300\n",
      "adana\t31295\n",
      "antakya\t30789\n",
      "acilyardim\t29822\n",
      "nurdagi\t29804\n",
      "turkeyquake\t27729\n",
      "mardin\t26861\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_n        = 50\n",
    "by_engagement = True  # False gives frequency, True gives engagement\n",
    "\n",
    "def parse_hashtags(cell):\n",
    "    if not cell:\n",
    "        return []\n",
    "    try:\n",
    "        tags = ast.literal_eval(cell)\n",
    "        if not isinstance(tags, list):\n",
    "            return []\n",
    "        seen = set()\n",
    "        for t in tags:\n",
    "            if isinstance(t, str):\n",
    "                tt = t.strip().lstrip(\"#\").lower()\n",
    "                if tt:\n",
    "                    seen.add(tt)\n",
    "        return list(seen)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "freq = Counter()\n",
    "weighted = Counter()\n",
    "\n",
    "with open(input_file1, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        tags = parse_hashtags(row.get(\"hashtags\", \"\"))\n",
    "\n",
    "        if not by_engagement:\n",
    "            freq.update(tags)\n",
    "        else: \n",
    "            try:\n",
    "                likes = float(row.get(\"like_count\") or 0)\n",
    "                rts   = float(row.get(\"rt_count\") or 0)\n",
    "            except ValueError:\n",
    "                likes = rts = 0.0\n",
    "            weight = likes + rts\n",
    "            if tags and weight > 0:\n",
    "                for t in tags:\n",
    "                    weighted[t] += weight\n",
    "\n",
    "if not by_engagement:\n",
    "    print(\"Top Hashtags by Frequency\")\n",
    "    print(\"hashtag\\tcount\")\n",
    "    for tag, c in freq.most_common(top_n):\n",
    "        print(f\"{tag}\\t{c}\")\n",
    "else:\n",
    "    print(\"Top Hashtags by Engagement (likes+retweets)\")\n",
    "    print(\"hashtag\\tscore\")\n",
    "    for tag, s in weighted.most_common(top_n):\n",
    "        s_out = int(s) if abs(s - int(s)) < 1e-9 else f\"{s:.1f}\"\n",
    "        print(f\"{tag}\\t{s_out}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
