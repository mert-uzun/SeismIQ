{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8e1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime\n",
    "from datetime import time\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f32640",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file1 = r\"Aug10_earthquake_tweets.csv\"\n",
    "input_file2 = r\"Aug10_earthquake_tweets_media.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d97ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Words in Aug10_earthquake_tweets_media.csv\n",
      "deprem (2442)\n",
      "olsun (2315)\n",
      "bir (2045)\n",
      "balıkesir (2026)\n",
      "geçmiş (2021)\n",
      "ve (1838)\n",
      "meydana (1243)\n",
      "61 (1153)\n",
      "balıkesirde (1080)\n",
      "de (951)\n",
      "sındırgı (937)\n",
      "gelen (873)\n",
      "bu (805)\n",
      "tüm (760)\n",
      "her (605)\n",
      "etkilenen (560)\n",
      "vatandaşlarımıza (560)\n",
      "geldi (559)\n",
      "depremde (542)\n",
      "çok (539)\n",
      "depremden (539)\n",
      "can (528)\n",
      "yıkılan (523)\n",
      "hissedilen (517)\n",
      "bina (513)\n",
      "büyüklüğünde (510)\n",
      "en (495)\n",
      "büyüklüğündeki (474)\n",
      "oldu (464)\n",
      "sındırgıda (457)\n",
      "allah (438)\n",
      "da (432)\n",
      "sonrası (426)\n",
      "korusun (425)\n",
      "ne (424)\n",
      "olan (405)\n",
      "var (394)\n",
      "rabbim (391)\n",
      "için (375)\n",
      "kaybı (355)\n",
      "çevre (348)\n",
      "daha (347)\n",
      "merkez (342)\n",
      "balıkesirin (342)\n",
      "ilçesinde (327)\n",
      "şener (326)\n",
      "yine (316)\n",
      "1 (314)\n",
      "dileklerimizi (311)\n",
      "üssü (307)\n",
      "depremin (292)\n",
      "önce (280)\n",
      "yok (279)\n",
      "62 (275)\n",
      "altında (273)\n",
      "türlü (268)\n",
      "hissedildi (267)\n",
      "kişi (264)\n",
      "iletiyoruz (261)\n",
      "i̇stanbul (257)\n",
      "depremi (256)\n",
      "büyük (251)\n",
      "türkiye (251)\n",
      "enkaz (246)\n",
      "gsm (243)\n",
      "milletimizi (238)\n",
      "km (231)\n",
      "illerde (231)\n",
      "üşümezsoy (231)\n",
      "yıkıldı (229)\n",
      "büyüklük (226)\n",
      "ama (226)\n",
      "başkanı (225)\n",
      "ülkemizi (223)\n",
      "saat (222)\n",
      "10 (221)\n",
      "devam (221)\n",
      "derinlik (220)\n",
      "kadar (214)\n",
      "belediye (213)\n",
      "olduğu (208)\n",
      "böyle (202)\n",
      "son (202)\n",
      "binanın (201)\n",
      "binalar (199)\n",
      "ediyor (196)\n",
      "merkezli (194)\n",
      "olarak (194)\n",
      "ile (194)\n",
      "beterinden (193)\n",
      "değil (193)\n",
      "gibi (191)\n",
      "88 (183)\n",
      "prof (182)\n",
      "herkese (181)\n",
      "şiddetinde (180)\n",
      "dr (179)\n",
      "sonra (178)\n",
      "dileklerimi (178)\n",
      "haber (177)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loads a CSV, \n",
    "cleans and analyzes its text to count top words\n",
    "\"\"\"\n",
    "\n",
    "text_column_name = 'text'\n",
    "top_n = 100\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'http\\S+|@\\w+|#\\w+', '', text, flags=re.MULTILINE)\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
    "    return cleaned_text.lower()\n",
    "\n",
    "def get_word_distribution(file_path, column_name):\n",
    "    word_counts = collections.Counter()\n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            if column_name not in reader.fieldnames:\n",
    "                print(f\"Error: The column '{column_name}' was not found in the file.\")\n",
    "                return None\n",
    "            for row in reader:\n",
    "                text = row.get(column_name, '')\n",
    "                if text:\n",
    "                    cleaned = clean_text(text)\n",
    "                    words = cleaned.split()\n",
    "                    word_counts.update(words)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        return None\n",
    "    return word_counts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    word_counts = get_word_distribution(input_file2, text_column_name)\n",
    "    if word_counts:\n",
    "        top_words = word_counts.most_common(top_n)\n",
    "        print(f\"Top {top_n} Words in {input_file2}\")\n",
    "        for word, count in top_words:\n",
    "            print(f\"{word} ({count})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127f1f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour       Posts in Aug10_earthquake_tweets_media.csv (%)\n",
      "0                                  4.80%\n",
      "1                                  2.11%\n",
      "2                                  0.95%\n",
      "3                                  0.46%\n",
      "4                                  0.61%\n",
      "5                                  0.63%\n",
      "6                                  0.73%\n",
      "7                                  0.10%\n",
      "8                                  0.00%\n",
      "9                                  0.00%\n",
      "10                                 0.00%\n",
      "11                                 0.00%\n",
      "12                                 0.00%\n",
      "13                                 0.00%\n",
      "14                                 0.00%\n",
      "15                                 0.00%\n",
      "16                                 0.00%\n",
      "17                                 0.00%\n",
      "18                                 0.00%\n",
      "19                                 2.96%\n",
      "20                                46.50%\n",
      "21                                20.77%\n",
      "22                                11.65%\n",
      "23                                 7.72%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reads a CSV file’s date column, counts posts per hour, calculates the percentage for each hour, \n",
    "and prints an hourly posting distribution\n",
    "\"\"\"\n",
    "date_column_name = 'created_at'\n",
    "\n",
    "def get_hour_distribution(file_path, column_name):\n",
    "    hour_counts = collections.Counter()\n",
    "    total_posts = 0\n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            if column_name not in reader.fieldnames:\n",
    "                print(f\"Error: The column '{column_name}' was not found in '{file_path}'.\")\n",
    "                return None, None\n",
    "            for row in reader:\n",
    "                date_str = row.get(column_name, '')\n",
    "                if date_str:\n",
    "                    try:\n",
    "                        dt_object = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S %z')\n",
    "                        hour = dt_object.hour\n",
    "                        hour_counts[hour] += 1\n",
    "                        total_posts += 1\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    except Exception:\n",
    "        return None, None\n",
    "    return hour_counts, total_posts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hour_counts1, total_posts1 = get_hour_distribution(input_file2, date_column_name)\n",
    "    if hour_counts1 and total_posts1:\n",
    "        print(f\"{'Hour':<10} {'Posts in ' + input_file2 + ' (%)':<30}\")\n",
    "        for hour in range(24):\n",
    "            percentage1 = (hour_counts1.get(hour, 0) / total_posts1) * 100\n",
    "            print(f\"{hour:<10} {percentage1:>28.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a27aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date            Posts in Aug10_earthquake_tweets_media.csv (%)\n",
      "2025-08-10                                  89.61%\n",
      "2025-08-11                                  10.39%\n",
      "Total posts in 'Aug10_earthquake_tweets_media.csv': 8752\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reads the CSV file’s created_at column, counts posts per day, calculates daily posting percentages, \n",
    "and prints the distribution\n",
    "\"\"\"\n",
    "date_column_name = 'created_at'\n",
    "\n",
    "def get_date_distribution(file_path, column_name):\n",
    "    date_counts = collections.Counter()\n",
    "    total_posts = 0\n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            for row in reader:\n",
    "                date_str = row.get(column_name, '')\n",
    "                if date_str:\n",
    "                    try:\n",
    "                        dt_object = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S %z')\n",
    "                        date_obj = dt_object.date()\n",
    "                        date_counts[date_obj] += 1\n",
    "                        total_posts += 1\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    except Exception:\n",
    "        return None, None\n",
    "    return date_counts, total_posts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    date_counts1, total_posts1 = get_date_distribution(input_file2, date_column_name)\n",
    "    if date_counts1 and total_posts1:\n",
    "        all_dates = sorted(date_counts1.keys())\n",
    "        print(f\"{'Date':<15} {'Posts in ' + input_file2 + ' (%)':<35}\")\n",
    "        for date_obj in all_dates:\n",
    "            percentage1 = (date_counts1.get(date_obj, 0) / total_posts1) * 100\n",
    "            print(f\"{str(date_obj):<15} {percentage1:>33.2f}%\")\n",
    "        print(f\"Total posts in '{input_file2}': {total_posts1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49540a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top TF-IDF Words in Aug10_earthquake_tweets_media.csv\n",
      "deprem                    (0.3552)\n",
      "olsun                     (0.3367)\n",
      "bir                       (0.2975)\n",
      "balıkesir                 (0.2947)\n",
      "geçmiş                    (0.2940)\n",
      "ve                        (0.2674)\n",
      "meydana                   (0.1808)\n",
      "61                        (0.1677)\n",
      "balıkesirde               (0.1571)\n",
      "de                        (0.1383)\n",
      "sındırgı                  (0.1363)\n",
      "gelen                     (0.1270)\n",
      "bu                        (0.1171)\n",
      "tüm                       (0.1106)\n",
      "her                       (0.0880)\n",
      "geldi                     (0.0822)\n",
      "etkilenen                 (0.0815)\n",
      "vatandaşlarımıza          (0.0815)\n",
      "depremde                  (0.0788)\n",
      "depremden                 (0.0784)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Computes TF-IDF scores for every word, \n",
    "and prints the top-scoring terms \n",
    "\"\"\"\n",
    "\n",
    "text_column_name = 'text'\n",
    "top_n = 20\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'http\\S+|@\\w+|#\\w+', '', text, flags=re.MULTILINE)\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
    "    return cleaned_text.lower()\n",
    "\n",
    "def get_text_from_file(file_path, column_name):\n",
    "    all_text = []\n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            if column_name not in reader.fieldnames:\n",
    "                print(f\"Error: The column '{column_name}' was not found in '{file_path}'.\")\n",
    "                return None\n",
    "            for row in reader:\n",
    "                text = row.get(column_name, '')\n",
    "                if text:\n",
    "                    cleaned = clean_text(text)\n",
    "                    all_text.append(cleaned)\n",
    "    except Exception:\n",
    "        return None\n",
    "    return \" \".join(all_text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    document1 = get_text_from_file(input_file2, text_column_name)\n",
    "    if document1:\n",
    "        corpus = [document1]\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = tfidf_matrix[0].toarray().flatten()\n",
    "        top_words = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        print(f\"Top TF-IDF Words in {input_file2}\")\n",
    "        for word, score in top_words:\n",
    "            print(f\"{word:<25} ({score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e530c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Hashtags by Engagement (likes+retweets)\n",
      "hashtag\tscore\n",
      "deprem\t631775\n",
      "balıkesir\t77836\n",
      "depremoldu\t37984\n",
      "sallandık\t27206\n",
      "afad\t26655\n",
      "balikesir\t17170\n",
      "sındırgı\t13196\n",
      "sondakika\t11548\n",
      "sondaki̇ka\t9147\n",
      "gazzeyei̇nsanikoridor\t8452\n",
      "istanbuldeprem\t4784\n",
      "gsm\t4408\n",
      "izmir\t4384\n",
      "bursa\t3786\n",
      "istanbul\t3006\n",
      "manisa\t2981\n",
      "i̇stanbul\t2005\n",
      "çöktü\t1986\n",
      "ankara\t1891\n",
      "izmirdeprem\t1835\n",
      "gazzeyeumutol\t1728\n",
      "earthquake\t1708\n",
      "gazze\t1523\n",
      "türkiyedekatliamvar\t1423\n",
      "balıkesirdeprem\t1421\n",
      "sokakhayvanlarısahipsizdeğildir\t1314\n",
      "geçmişolsun\t1197\n",
      "emekliultrafakir\t1184\n",
      "5000kısmiultramağdur\t1183\n",
      "hissettik\t1105\n",
      "chpkomisyondançik\t1077\n",
      "devletmitingyapmazgereğiniyapar\t845\n",
      "aysetokyazi̇cinadalet\t841\n",
      "türkiye\t797\n",
      "depremiunutmaunutturma\t676\n",
      "emekliyearazaminsanihaktır\t653\n",
      "yangın\t651\n",
      "afat\t581\n",
      "trump_und_putin\t545\n",
      "demokratie\t545\n",
      "gazzeaçlıktanölüyor\t534\n",
      "pazar\t491\n",
      "şenerüşümezsoy\t487\n",
      "gündem\t484\n",
      "amedspor\t482\n",
      "gaza\t437\n",
      "31temmuzcovidyasası\t431\n",
      "sındırgıdeprem\t421\n",
      "turkcell\t413\n",
      "pazartesi\t392\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Finds most common hashtags\n",
    "based on frequency or engagement\n",
    "and prints them\n",
    "\"\"\"\n",
    "\n",
    "top_n = 50\n",
    "by_engagement = True #False to give by frequency, True to give by engagement\n",
    "\n",
    "def parse_hashtags(cell):\n",
    "    if not cell:\n",
    "        return []\n",
    "    try:\n",
    "        tags = ast.literal_eval(cell)\n",
    "        if not isinstance(tags, list):\n",
    "            return []\n",
    "        seen = set()\n",
    "        for t in tags:\n",
    "            if isinstance(t, str):\n",
    "                tt = t.strip().lstrip(\"#\").lower()\n",
    "                if tt:\n",
    "                    seen.add(tt)\n",
    "        return list(seen)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "freq = Counter()\n",
    "weighted = Counter()\n",
    "\n",
    "with open(input_file2, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        tags = parse_hashtags(row.get(\"hashtags\", \"\"))\n",
    "        if not by_engagement:\n",
    "            freq.update(tags)\n",
    "        else:\n",
    "            try:\n",
    "                likes = float(row.get(\"like_count\") or 0)\n",
    "                rts = float(row.get(\"retweet_count\") or 0)\n",
    "            except ValueError:\n",
    "                likes = rts = 0.0\n",
    "            weight = likes + rts\n",
    "            if tags and weight > 0:\n",
    "                for t in tags:\n",
    "                    weighted[t] += weight\n",
    "\n",
    "if not by_engagement:\n",
    "    print(\"Top Hashtags by Frequency\")\n",
    "    print(\"hashtag\\tcount\")\n",
    "    for tag, c in freq.most_common(top_n):\n",
    "        print(f\"{tag}\\t{c}\")\n",
    "else:\n",
    "    print(\"Top Hashtags by Engagement (likes+retweets)\")\n",
    "    print(\"hashtag\\tscore\")\n",
    "    for tag, s in weighted.most_common(top_n):\n",
    "        s_out = int(s) if abs(s - int(s)) < 1e-9 else f\"{s:.1f}\"\n",
    "        print(f\"{tag}\\t{s_out}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
